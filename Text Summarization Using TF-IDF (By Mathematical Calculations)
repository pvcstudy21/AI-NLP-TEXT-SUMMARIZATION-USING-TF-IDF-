# import libraries

import math
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import PorterStemmer

nltk.download('all')

# Sample document
document ="""
Swords, more than just blades of metal, are whispers of history etched in cold steel. 
From the bronze thrusting spears of ancient Egypt to the razor-sharp katanas of feudal Japan, each curve and bevel tells a tale. 
They were instruments of both war and nobility, wielded by knights in shining armor and samurai in quiet contemplation. 
Though their reign as primary weapons has passed, swords remain symbols of power, precision, and a legacy forged in battlefields and whispered legends. 
Their allure, sharp as ever, beckons us to imagine the clash of steel, the clang of honor, and the echoes of a bygone era when a sword was more than just a weapon, it was an extension of the soul.
"""

#document = "India boasts a rich and diverse history that spans thousands of years, characterized by a tapestry of civilizations, empires, and cultural achievements. The ancient Indus Valley Civilization, one of the world's earliest urban cultures, flourished around 3300â€“1300 BCE, showcasing advanced city planning and trade networks."

#document = "Optacloud enables companies to automate their cloud savings by using ML-powered optimization to maximize performance across APIs and microservices (CPaaS, generative AI apps, etc). Companies can reduce cloud costs, improve latency & minimize carbon footprint at 0 upfront cost. Optacloud is cloud agnostic (AWS, GCP, Azure, Alibaba, DigitalOcean, etc) and integrates in less than a minute, without any extra engineering effort."

# 1. Tokenize the sentences
sentences = sent_tokenize(document)
total_documents = len(sentences)
print(sentences)

# 2. Create the Frequency matrix of the words in each sentence.
def _create_frequency_matrix(sentences):
    frequency_matrix = {}
    stopWords = set(stopwords.words("english"))
    ps = PorterStemmer()

    for sent in sentences:
        freq_table = {}
        words = word_tokenize(sent)
        for word in words:
            word = word.lower()
            word = ps.stem(word)
            if word in stopWords:
                continue

            if word in freq_table:
                freq_table[word] += 1
            else:
                freq_table[word] = 1

        frequency_matrix[sent[:15]] = freq_table

    return frequency_matrix

freq_matrix = _create_frequency_matrix(sentences)
print(freq_matrix)

# 3. Calculate TermFrequency and generate a matrix
def _create_tf_matrix(freq_matrix):
    tf_matrix = {}

    for sent, f_table in freq_matrix.items():
        tf_table = {}

        count_words_in_sentence = len(f_table)
        for word, count in f_table.items():
            tf_table[word] = count / count_words_in_sentence

        tf_matrix[sent] = tf_table

    return tf_matrix

tf_matrix = _create_tf_matrix(freq_matrix)
print(tf_matrix)

# 4. Creating a table for documents per words
def _create_documents_per_words(freq_matrix):
    word_per_doc_table = {}

    for sent, f_table in freq_matrix.items():
        for word, count in f_table.items():
            if word in word_per_doc_table:
                word_per_doc_table[word] += 1
            else:
                word_per_doc_table[word] = 1

    return word_per_doc_table

count_doc_per_words = _create_documents_per_words(freq_matrix)
print(count_doc_per_words)

# 5. Calculate IDF and generate a matrix
def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):
    idf_matrix = {}

    for sent, f_table in freq_matrix.items():
        idf_table = {}

        for word in f_table.keys():
            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))

        idf_matrix[sent] = idf_table

    return idf_matrix

idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)
print(idf_matrix)

# 6. Calculate TF-IDF and generate a matrix
def _create_tf_idf_matrix(tf_matrix, idf_matrix):
    tf_idf_matrix = {}

    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):

        tf_idf_table = {}

        for (word1, value1), (word2, value2) in zip(f_table1.items(),
                                                    f_table2.items()):  # here, keys are the same in both the table
            tf_idf_table[word1] = float(value1 * value2)

        tf_idf_matrix[sent1] = tf_idf_table

    return tf_idf_matrix

tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)
print(tf_idf_matrix)

# 7. Score the sentences
def _score_sentences(tf_idf_matrix) -> dict:
    """
    score a sentence by its word's TF
    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.
    :rtype: dict
    """

    sentenceValue = {}

    for sent, f_table in tf_idf_matrix.items():
        total_score_per_sentence = 0

        count_words_in_sentence = len(f_table)
        for word, score in f_table.items():
            total_score_per_sentence += score

        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence

    return sentenceValue

sentenceValue = _score_sentences(tf_idf_matrix)
print(sentenceValue)

# 8. Find the threshold
def _find_average_score(sentenceValue) -> int:
    """
    Find the average score from the sentence value dictionary
    :rtype: int
    """
    sumValues = 0
    for entry in sentenceValue:
        sumValues += sentenceValue[entry]

    # Average value of a sentence from original summary_text
    average = (sumValues / len(sentenceValue))

    return average

threshold = _find_average_score(sentenceValue)
print(threshold)